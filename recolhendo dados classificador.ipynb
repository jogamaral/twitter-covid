{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recolhendo os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessando API do Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo arquivo .txt com as chaves para acessar a API do Twitter e em seguida acessando a API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter-tokens.txt','r') as tfile:\n",
    "    consumer_key = tfile.readline().strip('\\n')\n",
    "    consumer_secret = tfile.readline().strip('\\n')\n",
    "    access_token = tfile.readline().strip('\\n')\n",
    "    access_token_secret = tfile.readline().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperando os Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo os termos de buscas, idioma e quantidade dos tweets recuperados assim como filtrando para n√£o recuperar retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_search = 'covid' + '-filter:retweets'\n",
    "\n",
    "cursor_tweets = tw.Cursor(api.search,\n",
    "                  q=query_search,\n",
    "                  lang='pt',tweet_mode='extended').items(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperando o dicion√°rio previamente criado com o as informa√ß√µes que ser√£o recolhidas dos tweets recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dict = {'full_text':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperando os tweets atrav√©s do cursor criado anteriormente e salvando-as no dicion√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in cursor_tweets:\n",
    "    for key in tweets_dict.keys():\n",
    "        try:\n",
    "            # para cada tweet recuperado tenta salva-lo no dic√≠onario em sua respectiva chave\n",
    "            tw_from_key = tweet._json[key]\n",
    "            # usa express√£o regular para remover links dos tweets\n",
    "            if key == 'full_text':\n",
    "                tw_from_key = re.sub(r\"http\\S+\", \"\", tw_from_key)\n",
    "                tw_from_key = re.sub(r\"@\\S+\", \"\", tw_from_key)\n",
    "            tweets_dict[key].append(tw_from_key)\n",
    "        except KeyError:\n",
    "            # caso algum tweet recuperado n√£o tenha algum dos dados procurados adiciona uma string vazia\n",
    "            tw_from_key = ''\n",
    "            tweets_dict[key].append('')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados do dicion√°rio em um dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A CUFA est√° em todo territ√≥rio nacional  e em ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a segunda onda de covid t√° vindo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meus vizinhos encontraram a cura pro Covid e e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Papa Francisco disse que \"fofocar √© uma praga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nem na quarentena fiquei sem sol. Por isso ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A CUFA est√° em todo territ√≥rio nacional  e em ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Os casos de covid est√£o aumentando na Cro√°ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>l√° n√£o tem covid.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>\"pandemia comunista\", \"ditadura do covid\", \"in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Quais s√£o os seus planos para este domingo? \\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_text\n",
       "0   A CUFA est√° em todo territ√≥rio nacional  e em ...\n",
       "1                    a segunda onda de covid t√° vindo\n",
       "2   Meus vizinhos encontraram a cura pro Covid e e...\n",
       "3    Papa Francisco disse que \"fofocar √© uma praga...\n",
       "4    Nem na quarentena fiquei sem sol. Por isso ac...\n",
       "..                                                ...\n",
       "95  A CUFA est√° em todo territ√≥rio nacional  e em ...\n",
       "96    Os casos de covid est√£o aumentando na Cro√°ci...\n",
       "97                              l√° n√£o tem covid.....\n",
       "98  \"pandemia comunista\", \"ditadura do covid\", \"in...\n",
       "99  Quais s√£o os seus planos para este domingo? \\n...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_df = pd.DataFrame.from_dict(tweets_dict)\n",
    "tw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os tweets em um arquivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df.to_csv('tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A CUFA est√° em todo territ√≥rio nacional  e em ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a segunda onda de covid t√° vindo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meus vizinhos encontraram a cura pro Covid e e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Papa Francisco disse que \"fofocar √© uma praga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nem na quarentena fiquei sem sol. Por isso ac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  sentiment\n",
       "0  A CUFA est√° em todo territ√≥rio nacional  e em ...          2\n",
       "1                   a segunda onda de covid t√° vindo          2\n",
       "2  Meus vizinhos encontraram a cura pro Covid e e...          1\n",
       "3   Papa Francisco disse que \"fofocar √© uma praga...          2\n",
       "4   Nem na quarentena fiquei sem sol. Por isso ac...          3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_data_clf.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tweets foram classificados da seguinte maneira:\n",
    "    1->negativo,\n",
    "    2->neutro,\n",
    "    3->positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56\n",
       "2    40\n",
       "3     4\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sentiment.value_counts().plot(kind='bar')\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver um grande desbalanceamento entre a categoria positiva e as demais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo Tweets duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda que na coleta dos dados tenham sido filtrados os retweets ainda assim foram recolhidos alguns tweets duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.full_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(['full_text'], inplace=True)\n",
    "df.full_text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando teste e treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando predictors e labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = train_set.drop('sentiment',axis=1)\n",
    "classe = train_set['sentiment'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando fun√ß√£o que ir√° pr√©-processar o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(instancia):\n",
    "    # Remove caracteres indesejados.\n",
    "    instancia = instancia.lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','').replace('\"','').replace(',','')\n",
    "    # Removendo palavras e termos frequentes que n√£o tem relev√¢ncia nos dados.\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [Preprocessing(i) for i in tweets.full_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Manos por favor, COVID n√£o tem uma cl√°usula assinada na qual ele infectar√° o indiv√≠duo APENAS se na reuni√£o houver mais de 5 pessoas\n",
      "Vejo gente criticando o bar at√© virar do avesso e reunindo com 4 pessoas que n√£o moram c ela.\n",
      "\n",
      "Aglomera√ß√£o √© mais quest√£o de chance de cont√°gio Z√©\n",
      "\n",
      "Depois: manos favor covid cl√°usula assinada infectar√° indiv√≠duo apenas reuni√£o 5 pessoas vejo gente criticando bar virar avesso reunindo 4 pessoas moram c aglomera√ß√£o quest√£o chance cont√°gio z√©\n"
     ]
    }
   ],
   "source": [
    "print('''Antes: {}\n",
    "\n",
    "Depois: {}'''.format(train_set.full_text.iloc[0],tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokeniza√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a biblioteca nltk para tokenizar os tweets. Ou seja, dividir uma string ou textos em uma lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manos',\n",
       " 'favor',\n",
       " 'covid',\n",
       " 'cl√°usula',\n",
       " 'assinada',\n",
       " 'infectar√°',\n",
       " 'indiv√≠duo',\n",
       " 'apenas',\n",
       " 'reuni√£o',\n",
       " '5',\n",
       " 'pessoas',\n",
       " 'vejo',\n",
       " 'gente',\n",
       " 'criticando',\n",
       " 'bar',\n",
       " 'virar',\n",
       " 'avesso',\n",
       " 'reunindo',\n",
       " '4',\n",
       " 'pessoas',\n",
       " 'moram',\n",
       " 'c',\n",
       " 'aglomera√ß√£o',\n",
       " 'quest√£o',\n",
       " 'chance',\n",
       " 'cont√°gio',\n",
       " 'z√©']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "tweet_tokenizer.tokenize(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o objeto que faz a vetoriza√ß√£o dos dados de texto.\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica o vetorizador nos dados de texto e retorna uma matriz esparsa:\n",
    "freq_tweets = vectorizer.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando um modelo de Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treino de modelo de Machine Learning:\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando com alguns tweets do set de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Suspeita de covid more',\n",
       " '  Os casos de covid est√£o aumentando na Cro√°cia. Rezando pra lenda n√£o pegar üôåüèΩüôåüèΩüò≠',\n",
       " 'Tem umas pessoas que eu olho os storys e fico pensando COMO que nao pegou covid ainda????????']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testes = list(test_set.iloc[1:4].full_text)\n",
    "testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os dados de teste em vetores de palavras:\n",
    "freq_testes = vectorizer.transform(testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  Suspeita de covid more\n",
      "           Predi√ß√£o: 1\n",
      "           Classifica√ß√£o humana: 1\n",
      "Tweet:   Os casos de covid est√£o aumentando na Cro√°cia. Rezando pra lenda n√£o pegar üôåüèΩüôåüèΩüò≠\n",
      "           Predi√ß√£o: 1\n",
      "           Classifica√ß√£o humana: 1\n",
      "Tweet: Tem umas pessoas que eu olho os storys e fico pensando COMO que nao pegou covid ainda????????\n",
      "           Predi√ß√£o: 2\n",
      "           Classifica√ß√£o humana: 1\n"
     ]
    }
   ],
   "source": [
    "# Fazendo a classifica√ß√£o com o modelo treinado:\n",
    "for t,p, c in zip (testes,modelo.predict(freq_testes),test_set.iloc[1:4].sentiment):\n",
    "    # t representa o tweet e p a predi√ß√£o de cada tweet e c a classifi√ß√£o humana.\n",
    "    print ('''Tweet: {}\n",
    "           Predi√ß√£o: {}\n",
    "           Classifica√ß√£o humana: {}'''.format(t, c,p)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Medindo a acur√°cia m√©dia do modelo:\n",
    "teste_tweets = [Preprocessing(i) for i in test_set.full_text]\n",
    "freq_teste_tweets = vectorizer.transform(teste_tweets)\n",
    "\n",
    "resultados = modelo.predict(freq_teste_tweets)\n",
    "classes = test_set.sentiment\n",
    "\n",
    "metrics.accuracy_score(classes,resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   1  2  All\n",
      "Real               \n",
      "1         9  2   11\n",
      "2         5  3    8\n",
      "3         1  0    1\n",
      "All      15  5   20\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(classes, resultados, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.82      0.69        11\n",
      "           2       0.60      0.38      0.46         8\n",
      "           3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.40      0.40      0.38        20\n",
      "weighted avg       0.57      0.60      0.57        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaoa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Medidas de valida√ß√£o do modelo:\n",
    "sentimento=['Positivo','Negativo','Neutro']\n",
    "print (metrics.classification_report(classes,resultados))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
